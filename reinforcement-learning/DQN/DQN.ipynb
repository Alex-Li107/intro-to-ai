{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specified-graph",
   "metadata": {},
   "source": [
    "# Deep Q-learning (DQN)\n",
    "\n",
    "This notebook describes the DQN reinforcement learning (RL) algorithm. \n",
    "\n",
    "\n",
    "## Environment \n",
    "\n",
    "We will train an agent to balance a vertical pole on a cart. This is a classic environment in RL, named CartPole. See the environment description [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to get more details about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5298a26-4553-437e-9079-8328494be3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.905222Z",
     "start_time": "2024-07-07T19:29:14.888722Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _c_internal_utils: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\Code\\PY\\intro-to-ai\\reinforcement-learning\\.venv\\lib\\site-packages\\matplotlib\\__init__.py:172\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32m~\\Documents\\Code\\PY\\intro-to-ai\\reinforcement-learning\\.venv\\lib\\site-packages\\matplotlib\\cbook.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_running_interactive_framework\u001b[39m():\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Return the interactive framework whose event loop is currently running, if\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    any, or \"headless\" if no event loop can be started, or None.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m        \"macosx\", \"headless\", ``None``.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _c_internal_utils: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import utils.envs, utils.seed, utils.buffers, utils.torch, utils.common\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-digit",
   "metadata": {},
   "source": [
    "Initialize a set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9c1c-d26b-4ef5-a92e-e055b12bdf1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.936223Z",
     "start_time": "2024-07-07T19:29:14.931724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEEDS = [1,2,3]\n",
    "t = utils.torch.TorchHelper()\n",
    "DEVICE = t.device\n",
    "OBS_N = 4               # State space size\n",
    "ACT_N = 2               # Action space size\n",
    "MINIBATCH_SIZE = 10     # How many examples to sample per train step\n",
    "GAMMA = 0.99            # Discount factor in episodic reward objective\n",
    "LEARNING_RATE = 5e-4    # Learning rate for Adam optimizer\n",
    "TRAIN_AFTER_EPISODES = 10   # Just collect episodes for these many episodes\n",
    "TRAIN_EPOCHS = 5        # Train for these many epochs every time\n",
    "BUFSIZE = 10000         # Replay buffer size\n",
    "EPISODES = 300          # Total number of episodes to learn over\n",
    "TEST_EPISODES = 1       # Test episodes after every train episode\n",
    "HIDDEN = 512            # Hidden nodes\n",
    "TARGET_UPDATE_FREQ = 10 # Target network update frequency\n",
    "STARTING_EPSILON = 1.0  # Starting epsilon\n",
    "STEPS_MAX = 10000       # Gradually reduce epsilon over these many steps\n",
    "EPSILON_END = 0.01      # At the end, keep epsilon at this value\n",
    "\n",
    "# Global variables\n",
    "EPSILON = STARTING_EPSILON\n",
    "Q = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-emerald",
   "metadata": {},
   "source": [
    "Create the Cartpole Environment from Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1437a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.951722Z",
     "start_time": "2024-07-07T19:29:14.937723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "# Create replay buffer\n",
    "# Create network for Q(s, a)\n",
    "# Create target network\n",
    "# Create optimizer\n",
    "def create_everything(seed):\n",
    "\n",
    "    utils.seed.seed(seed)\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env.reset(seed=seed) ####\n",
    "    test_env = gym.make(\"CartPole-v0\")\n",
    "    test_env.reset(seed=10+seed) ####\n",
    "    buf = utils.buffers.ReplayBuffer(BUFSIZE)\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    Qt = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    OPT = torch.optim.Adam(Q.parameters(), lr = LEARNING_RATE)\n",
    "    return env, test_env, buf, Q, Qt, OPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-broadcasting",
   "metadata": {},
   "source": [
    "Implementing a function to update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supposed-cycling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.967223Z",
     "start_time": "2024-07-07T19:29:14.953223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update a target network using a source network\n",
    "def update(target, source):\n",
    "    for tp, p in zip(target.parameters(), source.parameters()):\n",
    "        tp.data.copy_(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-light",
   "metadata": {},
   "source": [
    "Implementing a function for the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alien-horror",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.982722Z",
     "start_time": "2024-07-07T19:29:14.969221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create epsilon-greedy policy\n",
    "def policy(env, obs):\n",
    "\n",
    "    global EPSILON, Q\n",
    "\n",
    "    obs = t.f(obs).view(-1, OBS_N)  # Convert to torch tensor\n",
    "    \n",
    "    # With probability EPSILON, choose a random action\n",
    "    # Rest of the time, choose argmax_a Q(s, a) \n",
    "    if np.random.rand() < EPSILON:\n",
    "        action = np.random.randint(ACT_N)\n",
    "    else:\n",
    "        qvalues = Q(obs)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    \n",
    "    # Epsilon update rule: Keep reducing a small amount over\n",
    "    # STEPS_MAX number of steps, and at the end, fix to EPSILON_END\n",
    "    EPSILON = max(EPSILON_END, EPSILON - (1.0 / STEPS_MAX))\n",
    "    # print(EPSILON)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-billion",
   "metadata": {},
   "source": [
    "Function to update the evaluation network (and target network through the previous update function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "written-maximum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:14.998224Z",
     "start_time": "2024-07-07T19:29:14.984222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update networks\n",
    "def update_networks(epi, buf, Q, Qt, OPT):\n",
    "    \n",
    "    # Sample a minibatch (s, a, r, s', d)\n",
    "    # Each variable is a vector of corresponding values\n",
    "    S, A, R, S2, D = buf.sample(MINIBATCH_SIZE, t)\n",
    "    \n",
    "    # Get Q(s, a) for every (s, a) in the minibatch\n",
    "    qvalues = Q(S).gather(1, A.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Get max_a' Qt(s', a') for every (s') in the minibatch\n",
    "    q2values = torch.max(Qt(S2), dim = 1).values\n",
    "\n",
    "    # If done, \n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (0)\n",
    "    # If not done,\n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (1)       \n",
    "    targets = R + GAMMA * q2values * (1-D)\n",
    "\n",
    "    # Detach y since it is the target. Target values should\n",
    "    # be kept fixed.\n",
    "    loss = torch.nn.MSELoss()(targets.detach(), qvalues)\n",
    "\n",
    "    # Backpropagation\n",
    "    OPT.zero_grad()\n",
    "    loss.backward()\n",
    "    OPT.step()\n",
    "\n",
    "    # Update target network every few steps\n",
    "    if epi % TARGET_UPDATE_FREQ == 0:\n",
    "        update(Qt, Q)\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-tanzania",
   "metadata": {},
   "source": [
    "Train function to train a DQN agent in Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-intersection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:15.013724Z",
     "start_time": "2024-07-07T19:29:15.000224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Play episodes\n",
    "# Training function\n",
    "def train(seed):\n",
    "\n",
    "    global EPSILON, Q\n",
    "    print(\"Seed=%d\" % seed)\n",
    "\n",
    "    # Create environment, buffer, Q, Q target, optimizer\n",
    "    env, test_env, buf, Q, Qt, OPT = create_everything(seed)\n",
    "\n",
    "    # epsilon greedy exploration\n",
    "    EPSILON = STARTING_EPSILON\n",
    "\n",
    "    testRs = []\n",
    "    last25testRs = []\n",
    "    print(\"Training:\")\n",
    "    pbar = tqdm.trange(EPISODES)\n",
    "    for epi in pbar:\n",
    "\n",
    "        # Play an episode and log episodic reward\n",
    "        S, A, R = utils.envs.play_episode_rb(env, policy, buf)\n",
    "        \n",
    "        # Train after collecting sufficient experience\n",
    "        if epi >= TRAIN_AFTER_EPISODES:\n",
    "\n",
    "            # Train for TRAIN_EPOCHS\n",
    "            for tri in range(TRAIN_EPOCHS): \n",
    "                update_networks(epi, buf, Q, Qt, OPT)\n",
    "\n",
    "        # Evaluate for TEST_EPISODES number of episodes\n",
    "        Rews = []\n",
    "        for epj in range(TEST_EPISODES):\n",
    "            S, A, R = utils.envs.play_episode(test_env, policy, render = False)\n",
    "            Rews += [sum(R)]\n",
    "        testRs += [sum(Rews)/TEST_EPISODES]\n",
    "\n",
    "        # Update progress bar\n",
    "        last25testRs += [sum(testRs[-25:])/len(testRs[-25:])]\n",
    "        pbar.set_description(\"R25(%g)\" % (last25testRs[-1]))\n",
    "\n",
    "    # Close progress bar, environment\n",
    "    pbar.close()\n",
    "    print(\"Training finished!\")\n",
    "    env.close()\n",
    "\n",
    "    return last25testRs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-barrel",
   "metadata": {},
   "source": [
    "Running training and plotting performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-airfare",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:15.091225Z",
     "start_time": "2024-07-07T19:29:15.015223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot mean curve and (mean-std, mean+std) curve with some transparency\n",
    "def plot_arrays(vars, color, label):\n",
    "    mean = np.mean(vars, axis=0)\n",
    "    std = np.std(vars, axis=0)\n",
    "    plt.plot(range(len(mean)), mean, color=color, label=label)\n",
    "    plt.fill_between(range(len(mean)), np.maximum(mean-std, 0), np.minimum(mean+std,200), color=color, alpha=0.3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Train for different seeds\n",
    "    curves = []\n",
    "    for seed in SEEDS:\n",
    "        curves += [train(seed)]\n",
    "\n",
    "    # Plot the curve for the given seeds\n",
    "    plot_arrays(curves, 'b', 'dqn')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward (averaged over last 25 episodes)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-effectiveness",
   "metadata": {},
   "source": [
    "## Part 3: DQN \n",
    "\n",
    "a) Modify and run the code for CartPole DQN to produce a graph where the\n",
    "y-axis is the running average of 25 episodes for the cumulative rewards\n",
    "obtained at each episode and the x-axis is the number of episodes up\n",
    "to a minimum of 300 episodes. **The graph should contain 3 curves\n",
    "corresponding to updating the target network every 1, 10 (default) and 30 \n",
    "training step(s).** To reduce stochasticity in the results, report curves\n",
    "that are the average of atleast 3 runs of the given code (with different\n",
    "random seeds). Based on the results, **explain the impact of the target\n",
    "network and relate the target network to value iteration** (worth 20%).\n",
    "\n",
    "b) Modify and run the code for CartPole DQN to produce a graph where the\n",
    "y-axis is the running average of 25 episodes for the cumulative rewards\n",
    "obtained at each episode and the x-axis is the number of episodes up\n",
    "to a minimum of 300 episodes. **The graph should contain 3 curves\n",
    "corresponding to sampling mini-batches of 1, 10 (default) and 30\n",
    "experience(s) from the replay buffer.** To reduce stochasticity in the results,\n",
    "report curves that are the average of at least 3 runs of the given code (with\n",
    "different random seeds). Based on the results, **explain the impact of the\n",
    "replay buffer and explain the difference between using the replay buffer\n",
    "and exact gradient descent** (worth 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f380d8-3735-43e0-97fb-710fa2a21a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T19:29:15.092721Z",
     "start_time": "2024-07-07T19:29:15.092721Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
